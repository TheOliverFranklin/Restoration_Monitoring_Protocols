---
title: "Longitudinal Survey 2 Water Depths"
author: "Oliver Franklin & Nicci Zargarpour"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=11, fig.height=8.5, echo = TRUE)
if (!requireNamespace("pacman", quietly = TRUE)) {
  install.packages("pacman")}
pacman::p_load(DT, tidyverse, dplyr, plotly, ggplot2, htmlwidgets, moments, car, update=F)
```

```{r export functions and folder, echo = FALSE}
# functions to use for exporting results to output_folder (which is prompted in R when not in R environment)

#export figure (pdf for static ggplot plots, html for interactive plotly plots)
export_plot <- function(plot, filename) {
  if (inherits(plot, "ggplot")) {
    # Export ggplot as PDF
    ggsave(
      filename = file.path(output_folder, filename),
      plot = plot,
      device = "pdf",
      width = 11,
      height = 8.5
    )
  } else if (inherits(plot, "plotly")) {
    # Ensure the filename has .html extension
    html_filename <- file.path(output_folder, sub("\\.pdf$", ".html", filename))
    # Export plotly as a **single self-contained** HTML file
    saveWidget(
      widget = plot, 
      file = html_filename, 
      selfcontained = TRUE  # 
    )
  } else {
    stop("Unsupported plot type. Only ggplot and plotly objects are supported.")
  }
}

# Function to save a table
export_table <- function(table, filename) {
  write.csv(
    table,
    file = file.path(output_folder, filename),
    row.names = FALSE
  )
}

# Function to save summary as txt
export_summary <- function(summary_text, filename_base) {
  # Ensure summary_text is a character vector (it should already be, but just in case)
  summary_text <- as.character(summary_text)
    # Save as .txt
  txt_path <- file.path(output_folder, paste0(filename_base, ".txt"))
  writeLines(summary_text, con = txt_path)
}

# Examples of exporting individual results:
# Export a figure
# export_plot(ggplot_plot, "Thalweg Water Depth by Distance Upstream.pdf")
# export_plot(plotly_plot, "Thalweg Water Depth by Distance Upstream.html")
# 
# # Exporting a table
# export_table(as.data.frame(summary_table), "summary_table.csv")
# 
# Example of exporting a summary
# summary_text <- capture.output(summary(mtcars))
# export_summary(summary_text, "summary")

# below code to confirm an output folder is already specified, OR will request it is specified in R (interactively via readline)
# authors output folder: "~/Git/Core Monitoring/standardised protocols/data_tidier/Longitudinal"
# BUT if you want to knit, you will need to specify a default location (within the 'else' term)
if (interactive()) {
  while (!exists("output_folder") || !dir.exists(output_folder)) {
    output_folder <- readline(prompt = "Specify the output folder: ")
    if (!dir.exists(output_folder)) {
      tryCatch({
        dir.create(output_folder, recursive = TRUE)
        message("Created folder: ", output_folder)
      }, error = function(e) {
        message("Invalid folder. Please try again.")
        output_folder <- NULL
      })
    }
  }
} else {
  # Specify a default folder for non-interactive mode (e.g., knitting)
  output_folder <- "~/Git/Core Monitoring/standardised protocols/data_tidier/Longitudinal"
  if (!dir.exists(output_folder)) {
    dir.create(output_folder, recursive = TRUE)
    message("Default output folder created: ", output_folder)
  }
}
```
# Data import

For this script we import the tidied dataframe generated by the Data Wrangling script, which was exported to your output folder.

```{r import data, echo=FALSE}
importeddata <- file.path(output_folder, "Long_dataframe.csv")
df <- read.csv(importeddata)  

## if you need to specify a different location to import from, use below code:
# importeddata<-read.csv("~/Git/Core Monitoring/standardised protocols/data_tidier/Longitudinal/Long_dataframe.csv") # specify your file location & name here
# df<-data.frame(importeddata)
```
Our data analyses start by looking into the variability of water depth along the thalweg. Remember that we obtained depth measurements at standard intervals, and also at intermediate locations if we encountered additional pool outlets and pool maximum depths. Although the intermediate measurements will be important for our calculations of residual pool depths (see file *Longitudinal Survey 3 Residual Pools*), for now we will just use the standard interval measurements to get estimates of reach water depth variability.

```{r standard interval depths, echo = FALSE}
standard.df<- df[!grepl("X", df$Location_Code), ]# this removes rows for which we included an "X" in the location code column
```

# Depth Variability

We will create a simple plot of water depth by distance along the reach for each site and each year, and will export this plot to your output folder. Note that water depth is distinct from bed elevation, and in reality neither the bed nor the water surface are horizontal over the course of a reach (they slope down).

Before proceeding with this section, think again about your hypotheses and predictions: What restoration action did you do, and how do you expect it to impact the variability of water depth? What was the mechanism and in what direction will variability change? It is essential that you consider and clarify these expectations now, because our brains are fantastic at post-hoc justifications. 

```{r depth by distance plots, echo = FALSE}
# Ensure the 'year' is treated as a factor with ordered levels (from older to newer)
standard.df$year <- as.factor(standard.df$year)
standard.df$year <- factor(standard.df$year, levels = sort(unique(standard.df$year), decreasing = TRUE))

# Define the facet labels as Site and Year
standard.df$facet_label <- with(standard.df, paste(Site, ifelse(is.na(year), "Unknown Year", as.character(year)), sep = " - "))

# Create a ggplot to plot the data
ggplot_plot<-ggplot(standard.df, aes(x = distance, y = -1 * Thalweg_Depth_m, color = Site)) +
  geom_line(linewidth = 1) +  # Add lines for each data point
  facet_wrap(Site ~ year, scales = "fixed", ncol = 1) +  # Facet by Site and Year, fix y-axis scale
  labs(
    x = "Distance (m)",
    y = "Water Depth (m)"
  ) +
  theme_minimal() +  # Minimal theme for better visual appeal
  theme(
    strip.text = element_text(size = 12),  # Increase the size of facet labels
    axis.text = element_text(size = 10),   # Adjust axis label sizes
    legend.position = "none"               # Remove the legend
  )
plotly_plot<-ggplotly(ggplot_plot) 
plotly_plot

# Export figure as both static and interactive
export_plot(ggplot_plot, "Thalweg Water Depth by Distance Upstream.pdf")
export_plot(plotly_plot, "Thalweg Water Depth by Distance Upstream.html")
```

The plots above should allow some insight into differences in depth variation among your reaches and/or years. 

## Matching Surveys

Depending on your choice of reaches or other factors that can come into play, the distance over which each survey was conducted may not match perfectly - for example if you wanted to capture a certain feature, or if the best reference reach was wider than your restored reach. You will have to consider whether these differences could impact depth variability independently of your restoration actions. If your reaches are otherwise well-matched but differ slightly in width (and therefore distance surveyed) then variability metrics are likely still valid as long as the number of depth measurements are the same. If you measured a different number of *standard* intervals at different locations or on different years, we will need to create subsets of the surveys with equal numbers of depth measurements before proceeding. We will do this now. If your surveys are of equal length, this section will not have an impact.

For those surveys that included more intervals, we select the subset beginning from the start (downstream) end. This might not correspond to a particular feature of interest (the reason you took more measurements). If you want to subset a particular different area, the simplest approach is to manually create a copy of your raw data .csv with only your chosen subset of intervals, and then return to the Data Wrangling step. However, if you do this, ensure you have a valid reason to choose that particular subset, otherwise this is a very effective way to introduce bias. Note that while you must use matching intervals for this script, it is not deemed necessary for the other scripts in this package (the other scripts are based on the the full surveys). As such, if you are manually subsetting your raw data, consider which subset you upload into each script.

```{r match lengths of surveys, echo = FALSE}
# Step 1: Exclude rows with "X" in the Location_Code column (case-insensitive) - we want to use only the standard intervals here
standard.df_clean <- standard.df %>%
  filter(!grepl("X", Location_Code, ignore.case = TRUE))  # Exclude rows where Location_Code contains "X"

# Step 2: Identify the minimum number of rows (after excluding "X") by site and year
min_rows <- standard.df_clean %>%
  group_by(Site, year) %>%
  summarise(num_rows = n(), .groups= "drop") %>%
  ungroup() %>%
  summarise(min_rows = min(num_rows)) %>%
  pull(min_rows)

# Step 3: Subsample all sites and years to match the minimum number of rows
standard.df_subsampled <- standard.df_clean %>%
  group_by(Site, year) %>%
  slice(1:min_rows) %>%  # Subsample to the minimum number of rows
  ungroup()  # Remove grouping
```

# Variability Metrics

Before proceeding with the variability metrics, take another look at the plots above. Unlike bed elevation data, with water depths we cannot immediately see which features are pools and where the pool outlets are. This is because, without local water slope information, we cannot distinguish how far a particular pool outlet would backwater upstream. Fortunately, we can estimate this using a value of the water surface slope, and we will do this when we focus on residual pools. For now, we will examine the heterogeneity of depth, which itself can be an important metric of a reach's condition.

Water depth is of course sensitive to flow conditions during your survey, but *variability in water depth* is considered a flow-independent index of complexity (e.g. Kaufman et al 1999). We recommend withholding interpretation of *absolute* depth values until we consider the (flow-independent) residual pool depths in *Longitudinal Survey 3 Residual Pools*.

Below we generate the following three metrics for comparing thalweg water depth data:

- **Standard deviation (SD)**: SD can be interpreted in terms of the absolute variability of the data. It is expressed in metres and is probably the most familiar variability metric. We expect SD to be the most suitable variability metric when comparing the same type of habitat measured at similar spatial scales, for example when comparing changes over time at one location, or comparing well-matched reference/control reaches. We recommend using this metric unless you have good reason to do otherwise.

- **Coefficient of variation (CV)**: CV is normalised by dividing SD by the mean, giving a % variability. It is slightly more abstract but may be useful in comparing among distinct habitats / times, where the difference in overall depth is of significance. For example, if we assume the variability of depth is equal in two streams, but one was 0.2 m deep on average, while the other was 1 m deep, then the SD can be the same but the CV would differ.   

- **Mean square error (MSE)** of a linear regression of depth on distance: This approach has been used to explore variation in stream reaches where elevation data (rather than water depth data) is collected (e.g., Mossop and Bradford 2006). It is more sensitive to outliers (very deep / shallow values), and it reflects how much the observed thalweg variability differs from a straight line that is angled to fit the data. Because we have collected depth data, our data does not include an inherent slope that needs to be 'controlled' by fitting a regression line. However, we still include this for potential comparisons with elevation-derived data (though be *very* cautious if comparing among data collected in different ways).

There are many other metrics that can be applied to this data, each with their own advantages and disadvantages (e.g., Loess smoothing, fractal dimensions, wiggliness, see Bartley and Rutherfurd 2005 for a useful comparison). If a particular metric is common for your study system or species-of-interest, or there are demonstrated instances of its relationship to your ultimate restoration goal, it is probably worthwhile considering it. Although use of more complex/powerful/recent metrics can be tempting and should be encouraged, we do caution against the potential to 'try out' a number of different metrics, which can result in selecting the particular metrics which confirm your existing assumptions. 

Since we want to keep things accessible and broadly applicable, we recommend the use of standard deviation of water depths as a metric of overall reach depth heterogeneity. It is always useful to spend time examining the visualisation of the data, and interpreting your metrics not in isolation, but alongside the other metrics we will generate later. For example, two streams with similar standard deviations might be distinguished based on an increased depth of pools, or, two streams with equally deep pools might have different standard deviations due to variability of shallow habitats. 

The below table summarises variability by site and year, and is exported to your output folder.

```{r water depth heterogeneity, echo = FALSE}
variability_stats <- standard.df_subsampled %>%
  group_by(Site, year) %>%
  summarise(
    SD = sd(Thalweg_Depth_m, na.rm = TRUE),        # Standard deviation
    CV = SD / mean(Thalweg_Depth_m, na.rm = TRUE),  # Coefficient of Variation
    MSE = mean((Thalweg_Depth_m - predict(lm(Thalweg_Depth_m ~ distance, data = .), newdata = .))^2, na.rm = TRUE),
    .groups="drop"
  )

# View the variability statistics
DT::datatable(
  variability_stats,
  caption = "Thalweg Depth Variability Metrics by Site and Year",
  rownames = FALSE,   # Hide row numbers
  options = list(
    pageLength = 10,  # Number of rows per page
    autoWidth = TRUE  # Automatically adjust column width
  )
)%>%
  DT::formatRound(columns = c("SD", "CV", "MSE"), digits = 3)  # Round specific numeric columns to 3 decimal places

# Export table
export_table(as.data.frame(variability_stats), "Depth Variability by Site-Year.csv")
```

The simple plot below visually contrasts the mean and standard deviation for multiple years and/or multiple sites (where available). Remember that, since water depth is flow dependent, focus primarily on interpreting the standard deviation (the bar magnitude) rather than the mean (the vertical location of the bar/mean).

```{r depth SD by site plot, echo = FALSE}
# Calculate mean and SD for each Site and year
depth_stats <- standard.df_subsampled %>%
  group_by(Site, year) %>%
  summarise(
    mean_depth = mean(Thalweg_Depth_m, na.rm = TRUE),
    sd_depth = sd(Thalweg_Depth_m, na.rm = TRUE),
    .groups = "drop"
  )

# Ensure that Site is a factor and years are sorted in ascending order
depth_stats$Site <- factor(depth_stats$Site, levels = unique(depth_stats$Site))  # Keep site order as is
depth_stats$year <- factor(depth_stats$year, levels = rev(sort(unique(depth_stats$year))))  # Sort years in order

# Create a combined Site_Year factor variable to maintain correct ordering
depth_stats$Site_Year <- with(depth_stats, interaction(Site, year, sep = "_"))

# Ensure Site_Year is treated as a factor with correct levels
depth_stats$Site_Year <- factor(depth_stats$Site_Year, 
                                 levels = unique(depth_stats$Site_Year[order(depth_stats$Site, depth_stats$year)]))

# Create a scatter plot with error bars for mean and SD
scatter_plot_with_error_bars <- plot_ly(
  data = depth_stats,
  x = ~Site_Year,  # Use the combined 'Site_Year' for the x-axis
  y = ~mean_depth,
  type = "scatter",
  mode = "markers",  # Markers only
  marker = list(color = "black", size = 10),  # Customize marker style
  error_y = list(
    type = "data",
    array = ~sd_depth,  # Add SD above and below the mean
    visible = TRUE,  # Show error bars
    color = "grey"  # Customize error bar color
  )
) %>%
  layout(
    title = "Mean and Standard Deviation of Thalweg Depth by Site and Year",
    xaxis = list(
      title = "Site and Year",
      tickangle = 45  # Rotate the x-axis labels to prevent overlap
    ),
    yaxis = list(
      title = "Thalweg Depth (m)",
      zeroline = TRUE  # Add baseline at zero
    ),
    showlegend = FALSE  # No legend needed for this simple plot
  )

# Show the plot
scatter_plot_with_error_bars

# Export figure
export_plot(scatter_plot_with_error_bars, "Thalweg Depth SD by Site-Year.html")
```

# Inferential Statistics

We now ask whether the differences in water depth variability among sites and/or across time are statistically significant. Remember, we are focusing on variability and not absolute values here (due to potentially differing flow conditions). Because we are interested primarily in how our restoration action impacted the reach, we need to define the periods 'before' and 'after' restoration. 

Enter your before and after years in the below code, you may have more than one year in each category. These categories should balance (equal number of years before as after), but they need not be consecutive years. If you do not have balanced data (e.g., only one year before project implementation, but three years after) be very cautious in running and interpreting an ANOVA. You could select only a subset of your data to keep the comparison balanced, but if you do this for more than one set of dates you must account for the multiple comparisons inflating the likelihood of a Type I error (false positive).

```{r define before after}
# Define "Before" and "After" years
before_years <- c(2024) #add more years inside brackets if needed, e.g. 'c(2024, 2025, 2026)'
after_years <- c(2025) #add more years inside brackets if needed, e.g. 'c(2027, 2029, 2036)'
```

```{r categorise by before after, echo = FALSE}
# Categorize years
standard.df_subsampled <- standard.df_subsampled %>%
  mutate(Time_Category = case_when(
    year %in% before_years ~ "Before",
    year %in% after_years ~ "After",
    TRUE ~ NA_character_
  ))

#also remove any sites that do not have before/after data (partial data is useful for visualisation, but can bias tests)
standard.df_subsampled <- standard.df_subsampled %>%
  group_by(Site) %>%
  filter(all(c("Before", "After") %in% Time_Category)) %>%
  ungroup()
```

Now we have established the comparison groups, and the next step is to confirm that we are using the appropriate statistical test. You may have to transform your data or consider alternative tests if the distribution of your data does not meet the assumptions of the statistical tests we use.

Because we are interested in comparing variability, certain tests that are common among ecologists may be unsuitable (e.g., ANOVA assumes equal variances across groups). We will instead use a variant of Levene's test, called the Brown-Forsythe test, because this test compares the variance of groups (regardless of the absolute values of the averages) and is robust to non-normality and presence of outliers (because it uses the median, not the mean).

## Data-Test Suitability

Although the Brown-Forsythe test is relatively robust, we should still take a look at the data distribution. This next section generates tests and figures that you should review to ensure your data are suitable for an ANOVA. In brief, look for the following:

- Boxplots: consider whether outliers (dots) are real or errors. 
- Q-Q plots: a reasonably straight line of points means alignment with normal distribution
- Normality test (Shapiro-Wilk): p<0.05 indicates non-normality
- Skewness values < 0 are left skewed, > 0 right skewed. Kurtosis values < 3 light-tailed, > 3 heavy-tailed.

```{r normality skew etc, echo = FALSE}
# Ensure Time_Category is ordered correctly
standard.df_subsampled <- standard.df_subsampled %>%
  mutate(
    Time_Category = factor(Time_Category, levels = c("Before", "After")), # Explicitly order Time_Category
    Site_Time_Category = factor(
      paste(Site, Time_Category, sep = "_"),
      levels = unique(paste(Site, Time_Category, sep = "_")[
        order(Site, Time_Category == "After")
      ])
    )
  )

# boxplots can help identify outliers. While extreme values may not be unusual in your reach, mistakes in data entry might be identified here.
print(ggplot(standard.df_subsampled, aes(x = Site_Time_Category, y = Thalweg_Depth_m)) +
  geom_boxplot() +
  labs(title = "Boxplot of Thalweg Depth by Site and Time Category",
       x = "Site-Time Category", y = "Thalweg Depth (m)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)))

#Q-Q plots can visually examine normality (normal distributions resemble reasonably straight lines, with no curving, or tapering at the ends)
print(ggplot(standard.df_subsampled, aes(sample = Thalweg_Depth_m)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~ Site + Time_Category))

#Shapiro-Wilk test. If p<0.05, data is probably non-normal.
standard.df_subsampled %>%
  group_by(Site, Time_Category) %>%
  summarise(p_value = shapiro.test(Thalweg_Depth_m)$p.value, .groups = "drop")

#skewness and kurtosis. Skewness values < 0 are left skewed, > 0 right skewed. Kurtosis values < 3 light-tailed, > 3 heavy-tailed.
standard.df_subsampled %>%
  group_by(Site, Time_Category) %>%
  summarise(
    Skewness = skewness(Thalweg_Depth_m, na.rm = TRUE),
    Kurtosis = kurtosis(Thalweg_Depth_m, na.rm = TRUE),
    .groups = "drop")
```
It can also be useful to examine scatterplots of your data. If you notice any particular trends (e.g., decreasing depth with distance upstream), consider what this might mean for your interpretation. What observations did you make in the field that might explain this? Trends that are unexpected might need some more consideration so that you can understand whether these unknown trends might cause differences in the statistical results among reaches/years.

```{r any trends, echo = FALSE}
#trends - depth vs distance scatterplots
ggplot(standard.df_subsampled, aes(x = distance, y = Thalweg_Depth_m)) +
  geom_point() +
  facet_wrap(~ Site + Time_Category) +
  theme_minimal()
```

# Brown-Forsythe Test

It is quite likely that your data is non-normally distributed and features some skewness and kurtosis. It is difficult to assign a cut-off value to recommend transformations (e.g. log, or square root) but since this is a relatively robust test, if nothing jumps out at you as highly irregular, we will continue as is. 

Below we run the tests for site, time, and the site-time interaction, and also export these to your output folder. The test for Site only (or time only) considers whether variability observed at one site (time) differs significantly from variability at other sites (time). The null hypothesis is that variability is consistent across all sites (times), while the alternative hypothesis is that variability in depth differs for at least one site (time). If your p value is <0.05 (or your chosen risk tolerance) then the variance does differ significantly.

The test for the Site and Year interaction considers whether variability changes observed at one site are significantly different from variability changes observed at other sites (e.g., did our restoration site variability change differently to the control, consistent with us having an impact?). The null hypothesis is that variability is consistent across all site-year combinations, while the alternative hypothesis is that variability in depth differs for at least one site-year combo. If the returned p value is <0.05 (or your chosen risk tolerance) then the variance does differ significantly.

```{r Brown-Forsythe for Site only and Time Category only, echo = FALSE}
# Exclude rows with NA values in relevant columns
filtered_df <- standard.df_subsampled %>%
  filter(!is.na(Thalweg_Depth_m), !is.na(Site), !is.na(Time_Category))
# Ensure Site is a factor
filtered_df <- filtered_df %>%
  mutate(Site = factor(Site))
# Run the test for Site only
bf_test_site <- leveneTest(Thalweg_Depth_m ~ Site, data = filtered_df, center = median) #median makes it BF (not levene)
cat("\nBrown-Forsythe Test for Site:\n")
print(bf_test_site)
# Run the test for Time Category only
bf_test_time <- leveneTest(Thalweg_Depth_m ~ Time_Category, data = filtered_df, center = median) #median makes it BF (not levene)
cat("\nBrown-Forsythe Test for Time Category:\n")
print(bf_test_time)
```
```{r Brown-Forsythe Site and Year, echo = FALSE}
bf_test_site_time <- leveneTest(Thalweg_Depth_m ~ interaction(Site, Time_Category),
                                data = filtered_df, center = median)
cat("\nBrown-Forsythe Test for Site*Time Interaction:\n")
print(bf_test_site_time)

# export test summaries
site_output <- capture.output(print(bf_test_site))  # Capture the printed output of bf_test_site
time_output <- capture.output(print(bf_test_time))  # Capture the printed output of bf_test_time
sitetime_output <- capture.output(print(bf_test_site_time))  # Capture the printed output of bf_test_site_time
summary_text <- c("Brown-Forsythe Test for Site:", site_output, "\n\n\n", "Brown-Forsythe Test for Time Category:", time_output,"\n\n\n",  "Brown-Forsythe Test for Site-Time Interaction:", sitetime_output)
export_summary(summary_text, "Brown-Forsythe Depth Variability Tests")
```
So by now we either have some differences in depth variability that appear not to be due to chance, or we have no such detectable differences. Hopefully, any differences are relatively clear based on your data visualisations above (e.g., looking at the box-plot). However, if you are unsure which site/time the statistically-significant difference is attributable to (e.g., if you had multiple control/reference sites), you can run some post-hoc tests as below.

```{r posthoc tests}
pairwise_test <- pairwise.t.test(
  x = standard.df_subsampled$Thalweg_Depth_m,
  g = standard.df_subsampled$Site, 
  p.adjust.method = "bonferroni"
)
print(pairwise_test)
```
The above compares each pair of sites to see which differ (significant p value = yes they differ).

Once you have considered what the variability in water depth means for your restoration project, we can move on to *Longitudinal Survey 3 Residual Pools* to look in depth at the quantity and quality of pool habitat.

# References 

Bartley, R. and Rutherfurd, I., 2005. Measuring the reach‐scale geomorphic diversity of streams: Application to a stream disturbed by a sediment slug. River research and Applications, 21(1), pp.39-59.

Kaufmann, P.R., Levine, P., Peck, D.V., Robison, E.G. and Seeliger, C., 1999. Quantifying physical habitat in wadeable streams (p. 149). USEPA [National Health and Environmental Effects Research Laboratory, Western Ecology Division].

Mossop, B. and Bradford, M.J., 2006. Using thalweg profiling to assess and monitor juvenile salmon (Oncorhynchus spp.) habitat in small streams. Canadian Journal of Fisheries and Aquatic Sciences, 63(7), pp.1515-1525.

